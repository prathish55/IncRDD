package Test;

import java.io.File;
import java.io.Serializable;
import java.util.Collection;
import java.util.Comparator;

import org.apache.commons.io.FileUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.spark.Partitioner;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;

object Sampling {
	//var myRDD 
    def PrepareToSplit() : Tuple2<String,String> = {
               String[] parts = StringUtils.splitPreserveAllTokens(l, ",");
               String yrMoDd = parts[0]+","+parts[1]+","+parts[2];
			   myRDD = myRDD.add(new Tuple2<String,String>(yrMoDd,l))
               return new Tuple2<String,String>(yrMoDd,l);      
    }

    
    def main(args: Array[String]) {
        //System.out.println(System.getProperty("hadoop.home.dir"));
        
        val inputPath = args(0);
        val outputPath = args(1);
        val sampledAmt = args(2).toFloat;
        
        /*Identify the original number of partitions*/
        String[] extensions = {"bz2"};        
        Collection<File> files = FileUtils.listFiles(new File(inputPath), extensions, false);
        int noOfPartitions = files.size();
        
        /*Delete output file. Do not do this in Production*/        
        FileUtils.deleteQuietly(new File(outputPath));
        
        /*Initialize Spark Context*/
        JavaSparkContext sc = new JavaSparkContext("local", "airlinedatasampler");
        /*Read in the data*/
        JavaRDD<String> rdd = sc.textFile(inputPath);
        val data = sc.parallelize("a","abc")
	var myRDD = IncRDD(data).cache()
        /*Process the data*/
        rdd.filter(new IsHeader()) //Skip the header line
                       .sample(false, sampledAmt) //Sample each file
                       //.repartition(20)                       
                       .mapToPair(PrepareToSplit()) //Map to key-value pair                    
        myRDD.repartitionAndSortWithinPartitions(
                               new CustomPartitioner(noOfPartitions), //Partition the output as the original input
                               new CustomComparator()) //Sort in ascending order by Year,Month and Day
                       .map(t->t._2) //Process just the value
                       .saveAsTextFile(outputPath); //Write to file
        
        /*Close the context*/
        sc.close();
    }
}
